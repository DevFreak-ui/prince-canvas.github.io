<!DOCTYPE html>
<html>

<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <!--Main Scripts-->
    <link rel="stylesheet" type="text/css" href="./style.css">
    <script src="./script.js" defer></script>
    <!-- MatJax -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML">
    </script>
    <!--Jquery-->
    <script type="text/javascript" src="https://code.jquery.com/jquery-1.11.0.min.js" defer></script>
    <script type="text/javascript" src="https://code.jquery.com/jquery-migrate-1.2.1.min.js" defer></script>
    <!--Slick-->
    <link rel="stylesheet" type="text/css" href="utils/slick/slick.css"/>
    <link rel="stylesheet" type="text/css" href="utils/slick/slick-theme.css"/>
    <script type="text/javascript" src="utils/slick/slick.min.js" defer></script>
    <!--Favicon-->
    <link rel="shortcut icon" href="./favicon.ico" type="image/x-icon">
    <link rel="icon" href="./favicon.ico" type="image/x-icon">
    <!--Twenty Twenty Theme-->
    <link rel="stylesheet" href="utils/twentytwenty/twentytwenty.css" type="text/css" media="screen" />
    <script src="utils/twentytwenty/jquery.event.move.js" type="text/javascript" defer></script>
    <script src="utils/twentytwenty/jquery.twentytwenty.js" type="text/javascript" defer></script>
    <script src="utils/twentytwenty/imagesloaded.pkgd.js" type="text/javascript" defer></script>
    <title>From Limited Contexts to Rich Data: Elevating Twi NLP</title>
</head>

<body>
<div class="update-banner">
    <p>ðŸ“¢ Updates about this project will be shared here regularly. Please check back for the latest developments!</p>
</div>

<div class="toggle-container">
    <button id="toggleButton" class="inactive">Dark Mode</button>
    <div id="navigateAndButton">
    <a href=#menu class="scroll-link"><button id="jumpTo">Navigate</button></a>
    <div id="navigation-content">
    <div id="navigation">
        <span class="inline-heading-2"><a href=#header class="scroll-link">Return to Top</a></span>
        <span class="inline-heading"><a href=#Introduction class="scroll-link">1. Introduction</a></span>
        <span class="inline-heading"><a href=#Methods class="scroll-link">2. Methods</a></span>
        <p><a href=#DataAcquisition class="scroll-link">a. Data Acquisition</a></p>
        <p><a href=#Translation class="scroll-link">b. Translation</a></p>
        <p><a href=#Validation class="scroll-link">c. Validation</a></p>
        <span class="inline-heading"><a href=#Results class="scroll-link">3. Anticipated Results</a></span>
        <span class="inline-heading"><a href=#Impact class="scroll-link">4. Impact and Application</a></span>
        <span class="inline-heading"><a href=#Conclusion class="scroll-link">5. Conclusion</a></span>
        <span class="inline-heading"><a href=#References class="scroll-link">6. References</a></span>
    </div>
    </div>
    </div>
</div>

<div class="outer-container">
    <div class="header" id="header">
        <h1>From Limited Contexts to Rich Data</h1>
        <p class="subheading">Elevating Twi NLP through Diverse and Verified Datasets</p>
        <p class="authors">
            <span class="author" id="author1">Jackline Mireku<sup>1</sup></span>, 
            <span class="author" id="author2">Prince Mireku<sup>2</sup></span>
        </p>
        <p class="associations">
            <span class="associations"><sup>1</sup>University of Education, Winneba</span>
            <span class="associations"><sup>2</sup>ML Collective</span>
        </p>
    </div>

    <div class="tldr">
        <p>
        This study aims to create a scalable, high-quality Twi-English parallel corpus with wide domain coverage, 
        an update mechanism, and a human-in-the-loop evaluation pipeline.
        </p>
    </div>

    <div class="section" id="Introduction">
        <h2>Introduction</h2>
        <div class="subsection">
            <p>
            The efficacy of LLMs like GPT largely depends on the quality and diversity of their training datasets. 
            However, for languages such as Twi, there is a noticeable scarcity of high-quality, diverse datasets. 
            Preliminary research showed that datasets are often derived from limited contexts, such as religious texts, 
            which do not provide the breadth for developing robust language models.
            </p>
            <p>
            This hinders representation in multilingual LLMs and the development of downstream tools for education, 
            healthcare, and civic tech.
            </p>
        </div>
    </div>

    <div class="section" id="Methods">
        <h2>Methods</h2>
        <div class="subsection" id="DataAcquisition">
            <h3>Data Acquisition and Preprocessing</h3>
            <p>
            We source raw textual data from diverse domains, including literature, health, social media, and news articles. 
            Texts are cleaned and aligned at the sentence level. To address domain imbalance, we supplement underrepresented 
            categories with synthetically generated sentence pairs that simulate natural use cases.
            </p>
        </div>

        <div class="subsection" id="Translation">
            <h3>Translation</h3>
            <p>
            Initial translations are generated using large language models (LLMs) such as NLLB-200 and Gemini 1.5, 
            prompted with example-rich templates to improve fluency and contextual relevance. Google Translate will 
            also be used to aid in the translation quality.
            </p>
        </div>

        <div class="subsection" id="Validation">
            <h3>Expert Validation</h3>
            <p>
            Selected samples undergo final review via an interactive interface where linguists and fluent speakers 
            evaluate translations based on clarity, correctness, and cultural fit.
            </p>
        </div>
    </div>

    <div class="section" id="Results">
        <h2>Anticipated Results</h2>
        <div class="subsection">
            <ul>
                <li>High-quality translations: At least 80% of validated sentence pairs are expected to score above 4 out of 5 in human adequacy and fluency evaluations.</li>
                <li>Improved translation performance: Fine-tuned multilingual models are projected to achieve +3 to +6 BLEU improvement over JW300 on general-domain Twi-English tasks.</li>
                <li>Domain balance: The final dataset will span â‰¥5 distinct domains, with no more than 30% from any single category.</li>
                <li>Expert agreement: Human evaluation via our interface is expected to yield a Cohen's Kâ‰¥0.75.</li>
            </ul>
        </div>
    </div>

    <div class="section" id="Impact">
        <h2>Impact and Application</h2>
        <div class="subsection">
            <p>
            The dataset will directly support the development of machine translation systems, voice assistants, 
            and educational tools tailored to Twi speakers. In healthcare, it can improve patient-doctor communication 
            through accurate translation of medical instructions and consent forms.
            </p>
            <p>
            In governance, it can enable the delivery of public service announcements and legal information in a 
            language that over 9 million Ghanaians understand. By grounding NLP systems in local linguistic realities, 
            our work lays the foundation for inclusive AI in West Africa.
            </p>
        </div>
    </div>

    <div class="section" id="Conclusion">
        <h2>Conclusion</h2>
        <div class="subsection">
            <p>
            Our approach addresses key challenges in low-resource language development, namely, data scarcity, 
            limited contextual diversity, and lack of verification pipelines. By integrating human expertise with 
            automated translation and validation tools, we lay the groundwork for scalable, high-quality dataset creation.
            </p>
        </div>
    </div>

    <div class="section" id="References">
        <h2>References</h2>
        <div class="subsection">
            <ol>
                <li>Michael Adjeisah, et al. "Twi corpus: A massively twi-to-handful languages parallel bible corpus." 2020 IEEE Intl Conf on Parallel Distributed Processing with Applications.</li>
                <li>Å½eljko AgiÄ‡ and Ivan VuliÄ‡. "JW300: A wide-coverage parallel corpus for low-resource languages." Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.</li>
                <li>Paul Azunre, et al. "English-twi parallel corpus for machine translation." 2021.</li>
                <li>Dorothee Beermann, et al. "Developing a Twi (Asante) dictionary from Akan interlinear glossed texts." Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages, 2020.</li>
                <li>Philip Resnik, et al. "The bible as a parallel corpus: Annotating the 'book of 2000 tongues'." Computers and the Humanities, 1999.</li>
            </ol>
        </div>
    </div>

</div> <!--outer container-->

</body>
</html>
